{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install oolearning --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import oolearning as oo\n",
    "from helpers import DataFrameSelector, CustomLogTransform, ChooserTransform, CombineAgeHoursTransform, CombineCapitalGainLossTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.path.join(os.getcwd(), '../')\n",
    "csv_file = os.path.join(working_directory, 'census.csv')\n",
    "target_variable = 'income'\n",
    "positive_class = '>50K'\n",
    "negative_class = '<=50K'\n",
    "\n",
    "#target_mapping = {0: 'died', 1: 'lived'}  # so we can convert from numeric to categoric\n",
    "\n",
    "explore = oo.ExploreClassificationDataset.from_csv(csv_file_path=csv_file,\n",
    "                                                   target_variable=target_variable)\n",
    " #                                                  map_numeric_target=target_mapping)\n",
    "\n",
    "# look at data\n",
    "explore.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.numeric_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.categoric_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.plot_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since I will be cross-validating transformations e.g. GridSearchCV, it typically won't work to\n",
    "# one-hot-encode during cross-validation because the holdout fold will tend to have categoric values that\n",
    "# weren't found in the training folds, and therefore will break during transformation because it will encode\n",
    "# a value (i.e. add a column) that didn't exist in the training folds.\n",
    "# So, for this, we need to fit ALL data. Then, below if we have new data e.g. Kaggle, we have to apply\n",
    "# the same pipeline (i.e.  cat_encoding_pipeline.transform()\n",
    "# TODO: this breaks though if there are any categorical features with missing values in the final test/Kaggle set\n",
    "one_hot_transformer = oo.DummyEncodeTransformer(encoding=oo.CategoricalEncoding.ONE_HOT)\n",
    "transformed_data = one_hot_transformer.fit_transform(explore.dataset.drop(columns=target_variable))\n",
    "transformed_data[target_variable] = explore.dataset[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations & Transformation Tuning Parameters\n",
    "\n",
    "define the transformations we want to do, some transformations will have parameters (e.g. base of log tranform (or no transform), type of scaling, whether or not to add column combinations (e.g. age * hours-per-week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the pipeline for captail-gain/lost. \n",
    "\n",
    "We want to tune whether or not we should log transform. We need to do this after imputing but before scaling, so it needs to be it's own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_gain_loss_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(attribute_names=['capital-gain', 'capital-loss'])),\n",
    "        ('imputer', Imputer()),\n",
    "        # tune Log trasformation base (or no transformation); update: tuned - chose base e\n",
    "        ('custom_transform', CustomLogTransform(base=math.e)),\n",
    "        # tune \"net gain\" (have to do it after log transform; log of <=0 doesn't exist)\n",
    "        ('custom_cap_gain_minus_loss', CombineCapitalGainLossTransform(combine=True)),\n",
    "        # tune MinMax vs StandardScaler; we chose MinMax; update: tuned - chose MinMax\n",
    "        ('custom_scaler', ChooserTransform(base_transformer=MinMaxScaler())),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the pipeline for the rest of numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(attribute_names=['age', 'education-num', 'hours-per-week'])),\n",
    "        ('imputer', Imputer()),\n",
    "        # tune age * hours-per-week; update: tuned -chose not to include\n",
    "        #('combine_agehours', CombineAgeHoursTransform()),\n",
    "        # tune MinMax vs StandardScaler; update: tuned - chose MinMax\n",
    "        ('custom_scaler', ChooserTransform(base_transformer=MinMaxScaler())),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline that simply gets the categorical/encoded columns from the previous transformation (which used `oo-learning`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_categoricals = Pipeline([\n",
    "        ('append_cats', DataFrameSelector(attribute_names=one_hot_transformer.encoded_columns))  # already encoded\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the pipeline for combining all of the other pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pipelines\n",
    "transformations_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"cap_gain_loss_pipeline\", cap_gain_loss_pipeline),\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", append_categoricals),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the transformations to tune, below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below calculates the a standard value for `scale_pos_weight` based on the recommendation from http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "> Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = np.sum(explore.dataset[target_variable] == positive_class)\n",
    "n_negative = np.sum(explore.dataset.income == negative_class)\n",
    "scale_pos_weight_calc = n_negative / n_positive\n",
    "scale_pos_weight_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state=42,\n",
    "                      # original\n",
    "                      # learning_rate =0.1,\n",
    "                      # n_estimators=1000,\n",
    "                      # max_depth=5,\n",
    "                      # min_child_weight=1,\n",
    "                      # gamma=0,\n",
    "                      # subsample=0.8,\n",
    "                      # colsample_bytree=0.8,\n",
    "                      \n",
    "                      objective='binary:logistic',\n",
    "                      learning_rate=0.1,\n",
    "                      n_estimators=1000,\n",
    "                      max_depth=5,\n",
    "                      min_child_weight=1,\n",
    "                      gamma=0.1,\n",
    "                      subsample=0.8,\n",
    "                      colsample_bytree=0.8,\n",
    "                      scale_pos_weight=scale_pos_weight_calc,\n",
    "#                       reg_alpha=0.005,\n",
    "#                       reg_lambda=1,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    ('preparation', transformations_pipeline),\n",
    "    #('pca_chooser', ChooserTransform()),  # PCA option lost; didn't include\n",
    "    #('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning strategy according to https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform, expon\n",
    "model_param_dict = {\n",
    "    # 1st\n",
    "        'model__n_estimators': randint(200, 2000),\n",
    "        'model__max_depth': randint(3, 11),\n",
    "        'model__min_child_weight': randint(1, 7),\n",
    "    # 2nd\n",
    "        # 'model__gamma': uniform(0, .4),\n",
    "    # 3rd\n",
    "        # 'model__subsample': uniform(0.5, 0.4),\n",
    "        # 'model__colsample_bytree': uniform(0.5, 0.4),  \n",
    "    # 4th\n",
    "        # 'model__scale_pos_weight': randint(1, 5),\n",
    "        # 'model__reg_alpha': expon(loc=0.00, scale=1.00),\n",
    "        # 'model__reg_lambda': uniform(1, 5),\n",
    "    # final\n",
    "        # 'model__learning_rate': uniform(1, 10),\n",
    "        # 'model__n_estimators': randint(200, 2000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual hyper-parameters/options to tune for transformations.\n",
    "transformation_parameters = {\n",
    "         #'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],  # tune strategy\n",
    "         #'pca_chooser__base_transformer': [PCA(n_components=0.95, random_state=42), None],  # PCA vs not\n",
    "         #'preparation__cap_gain_loss_pipeline__custom_transform__base': [None, math.e],  # Log transform (base e) or not\n",
    "         #'preparation__cap_gain_loss_pipeline__custom_scaler__base_transformer': [MinMaxScaler(), StandardScaler()],\n",
    "         #'preparation__num_pipeline__custom_scaler__base_transformer': [MinMaxScaler(), StandardScaler()],\n",
    "         #'preparation__num_pipeline__combine_agehours__combine': [True, False],\n",
    "         #'preparation__cap_gain_loss_pipeline__custom_cap_gain_minus_loss__combine': [True, False]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {**transformation_parameters, **model_param_dict}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_roc_auc(y_true, y_score):\n",
    "#     return roc_auc_score(y_true=y_true,\n",
    "#                          # binary makes it so it converts the \"scores\" to predictions\n",
    "#                          y_score=[1 if x > 0.5 else 0 for x in y_score])\n",
    "\n",
    "scorer = make_scorer(roc_auc_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  transformed_data[target_variable].apply(lambda x: 1 if x == positive_class else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data[target_variable].values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting....')\n",
    "time_start = time.time()\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid_search = RandomizedSearchCV(estimator=full_pipeline,\n",
    "                                 param_distributions=param_grid,\n",
    "                                 n_iter=100,\n",
    "                                 cv=RepeatedKFold(n_splits=5, n_repeats=1),\n",
    "                                 scoring=scorer,\n",
    "                                 return_train_score=True,\n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=2)\n",
    "grid_search.fit(transformed_data.drop(columns=target_variable), y)\n",
    "time_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time: {}m'.format(round((time_end-time_start)/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, std_score, params in zip(cvres[\"mean_test_score\"], cvres[\"std_test_score\"], cvres[\"params\"]):\n",
    "    print('mean: {};\\tstd: {},\\tparams: {}'.format(round(mean_score, 5), round(std_score, 5), params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column order is based off of the pipeline and FeatureUnion\n",
    "    # cap_gain_loss_pipeline\n",
    "    # num_pipeline\n",
    "    # cat_pipeline\n",
    "features = ['capital-gain', 'capital-loss'] + ['age', 'education-num', 'hours-per-week'] + one_hot_transformer.encoded_columns\n",
    "importances = grid_search.best_estimator_.steps[1][1].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'feature': features, 'importance': importances}).sort_values(by=['importance'], ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
